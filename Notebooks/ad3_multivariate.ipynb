{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@author Nassir Mohammad\n",
    "\n",
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T19:22:16.085670Z",
     "start_time": "2021-08-02T19:22:16.078580Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "sys.path.append('../')\n",
    "sys.path.append('../scripts')\n",
    "\n",
    "import warnings\n",
    "from perception_nassir import Perception\n",
    "\n",
    "import dataframe_image as dfi\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scripts.utilities import apply_classifiers\n",
    "from scripts.utilities import get_file_data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scripts.rendering_functions import highlight_max, highlight_min\n",
    "\n",
    "image_save_path = ''\n",
    "image_save_switch = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper 1 datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note on creditcard.csv**  \n",
    "> The file `creditcard.csv` is not included in this repository under `data/ODDS_multivariate` due to its large size.  \n",
    "> Download it from: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud and place it inside `data/ODDS_multivariate/`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T19:24:54.498344Z",
     "start_time": "2021-08-02T19:24:52.418531Z"
    }
   },
   "outputs": [],
   "source": [
    "# make table for dataset, # dimensions, # samples, # percentage of anomalies\n",
    "\n",
    "base_path = \"../data/ODDS_multivariate/\"\n",
    "data_properties_df = None\n",
    "\n",
    "# loop over datasets in directory\n",
    "for file_name in os.listdir(base_path):\n",
    " \n",
    "    dataset_name = file_name.split('.')[0]\n",
    "    file_path = base_path + file_name\n",
    "\n",
    "    if dataset_name == \"creditcard\":\n",
    "        df_temp = pd.read_csv(file_path, low_memory=False)\n",
    "        X_original = df_temp.iloc[:, :-1].values.astype(float)\n",
    "        y = df_temp.iloc[:, -1].values.astype(float)\n",
    "    else:\n",
    "        dataset_name, X_original, y = get_file_data(base_path, file_name)\n",
    "\n",
    "    if dataset_name is None:\n",
    "        continue\n",
    "\n",
    "    # write dataset summary to dataframe\n",
    "    data_properties_temp = pd.DataFrame({\n",
    "        'Name': [dataset_name],\n",
    "        '# examples': [X_original.shape[0]],\n",
    "        '# features': [X_original.shape[1]],\n",
    "        # '# anomalies': [y.sum()],\n",
    "        '% anomalies': [round(y.sum() / X_original.shape[0] * 100, 2)],\n",
    "    })\n",
    "\n",
    "    data_properties_df = pd.concat(\n",
    "        [data_properties_df, data_properties_temp]).reset_index(drop=True)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T19:25:36.071578Z",
     "start_time": "2021-08-02T19:25:34.988414Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_991fa\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_991fa_level0_col0\" class=\"col_heading level0 col0\" >Name</th>\n",
       "      <th id=\"T_991fa_level0_col1\" class=\"col_heading level0 col1\" ># examples</th>\n",
       "      <th id=\"T_991fa_level0_col2\" class=\"col_heading level0 col2\" ># features</th>\n",
       "      <th id=\"T_991fa_level0_col3\" class=\"col_heading level0 col3\" >% anomalies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_991fa_row0_col0\" class=\"data row0 col0\" >cardio</td>\n",
       "      <td id=\"T_991fa_row0_col1\" class=\"data row0 col1\" >1831</td>\n",
       "      <td id=\"T_991fa_row0_col2\" class=\"data row0 col2\" >21</td>\n",
       "      <td id=\"T_991fa_row0_col3\" class=\"data row0 col3\" >9.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_991fa_row1_col0\" class=\"data row1 col0\" >creditcard</td>\n",
       "      <td id=\"T_991fa_row1_col1\" class=\"data row1 col1\" >284807</td>\n",
       "      <td id=\"T_991fa_row1_col2\" class=\"data row1 col2\" >30</td>\n",
       "      <td id=\"T_991fa_row1_col3\" class=\"data row1 col3\" >0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_991fa_row2_col0\" class=\"data row2 col0\" >http</td>\n",
       "      <td id=\"T_991fa_row2_col1\" class=\"data row2 col1\" >567498</td>\n",
       "      <td id=\"T_991fa_row2_col2\" class=\"data row2 col2\" >3</td>\n",
       "      <td id=\"T_991fa_row2_col3\" class=\"data row2 col3\" >0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_991fa_row3_col0\" class=\"data row3 col0\" >musk</td>\n",
       "      <td id=\"T_991fa_row3_col1\" class=\"data row3 col1\" >3062</td>\n",
       "      <td id=\"T_991fa_row3_col2\" class=\"data row3 col2\" >166</td>\n",
       "      <td id=\"T_991fa_row3_col3\" class=\"data row3 col3\" >3.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_991fa_row4_col0\" class=\"data row4 col0\" >satimage-2</td>\n",
       "      <td id=\"T_991fa_row4_col1\" class=\"data row4 col1\" >5803</td>\n",
       "      <td id=\"T_991fa_row4_col2\" class=\"data row4 col2\" >36</td>\n",
       "      <td id=\"T_991fa_row4_col3\" class=\"data row4 col3\" >1.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_991fa_row5_col0\" class=\"data row5 col0\" >shuttle</td>\n",
       "      <td id=\"T_991fa_row5_col1\" class=\"data row5 col1\" >49097</td>\n",
       "      <td id=\"T_991fa_row5_col2\" class=\"data row5 col2\" >9</td>\n",
       "      <td id=\"T_991fa_row5_col3\" class=\"data row5 col3\" >7.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_991fa_row6_col0\" class=\"data row6 col0\" >smtp</td>\n",
       "      <td id=\"T_991fa_row6_col1\" class=\"data row6 col1\" >95156</td>\n",
       "      <td id=\"T_991fa_row6_col2\" class=\"data row6 col2\" >3</td>\n",
       "      <td id=\"T_991fa_row6_col3\" class=\"data row6 col3\" >0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_991fa_row7_col0\" class=\"data row7 col0\" >thyroid</td>\n",
       "      <td id=\"T_991fa_row7_col1\" class=\"data row7 col1\" >3772</td>\n",
       "      <td id=\"T_991fa_row7_col2\" class=\"data row7 col2\" >6</td>\n",
       "      <td id=\"T_991fa_row7_col3\" class=\"data row7 col3\" >2.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_991fa_row8_col0\" class=\"data row8 col0\" >wbc</td>\n",
       "      <td id=\"T_991fa_row8_col1\" class=\"data row8 col1\" >378</td>\n",
       "      <td id=\"T_991fa_row8_col2\" class=\"data row8 col2\" >30</td>\n",
       "      <td id=\"T_991fa_row8_col3\" class=\"data row8 col3\" >5.56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x26c66ab96d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_title = \"Dataset properties\"\n",
    "path_save = image_save_path + \"dataset_properties.png\"\n",
    "\n",
    "# order the dataset rows by name\n",
    "data_properties_df = data_properties_df.sort_values(by=['Name']).reset_index(drop=True)\n",
    "\n",
    "data_properties_df_styled = data_properties_df.style.format({'% anomalies': \"{:.2f}\"}).hide()\n",
    "\n",
    "#data_properties_df_styled = data_properties_df.style.hide_index()\n",
    "\n",
    "#dfi.export(data_properties_df,path_save)\n",
    "\n",
    "data_properties_df_styled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T04:32:36.346095Z",
     "start_time": "2021-08-02T19:31:20.182967Z"
    },
    "code_folding": [
     73,
     83,
     96,
     106
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current file in progress ...: cardio\n",
      "current classifier in progress: HBOS\n",
      "total run time: 2.453952300013043\n",
      "current classifier in progress: HBOS\n",
      "total run time: 0.0037052001571282744\n",
      "current classifier in progress: IForest\n",
      "total run time: 0.1695482999784872\n",
      "current classifier in progress: KNN\n",
      "total run time: 1.9343896999489516\n",
      "current classifier in progress: LOF\n",
      "total run time: 0.3189489000942558\n",
      "current classifier in progress: MCD\n",
      "total run time: 0.3408673999365419\n",
      "current classifier in progress: OCSVM\n",
      "total run time: 0.3343865000642836\n",
      "current classifier in progress: Perception\n",
      "total run time: 0.0007828000234439969\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Time'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# scaling (very important to get right)\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# scale to zero mean and unit standard deviation along each feature\u001b[39;00m\n\u001b[32m     39\u001b[39m sc = StandardScaler(with_mean=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[43msc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_original\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m X = sc.transform(X_original)\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Apply each classifier to dataset\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\VCX22-013\\miniconda3\\envs\\perceptionenv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:924\u001b[39m, in \u001b[36mStandardScaler.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[32m    923\u001b[39m \u001b[38;5;28mself\u001b[39m._reset()\n\u001b[32m--> \u001b[39m\u001b[32m924\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\VCX22-013\\miniconda3\\envs\\perceptionenv\\Lib\\site-packages\\sklearn\\base.py:1336\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m     estimator._validate_params()\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1332\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m     )\n\u001b[32m   1335\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\VCX22-013\\miniconda3\\envs\\perceptionenv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:961\u001b[39m, in \u001b[36mStandardScaler.partial_fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    959\u001b[39m xp, _, X_device = get_namespace_and_device(X)\n\u001b[32m    960\u001b[39m first_call = \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mn_samples_seen_\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m961\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43msupported_float_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_device\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    969\u001b[39m n_features = X.shape[\u001b[32m1\u001b[39m]\n\u001b[32m    971\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\VCX22-013\\miniconda3\\envs\\perceptionenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2902\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2900\u001b[39m         out = X, y\n\u001b[32m   2901\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2902\u001b[39m     out = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2903\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2904\u001b[39m     out = _check_y(y, **check_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\VCX22-013\\miniconda3\\envs\\perceptionenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1022\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1020\u001b[39m         array = xp.astype(array, dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1021\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1022\u001b[39m         array = \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[32m   1024\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1025\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.format(array)\n\u001b[32m   1026\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcomplex_warning\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\VCX22-013\\miniconda3\\envs\\perceptionenv\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:878\u001b[39m, in \u001b[36m_asarray_with_order\u001b[39m\u001b[34m(array, dtype, order, copy, xp, device)\u001b[39m\n\u001b[32m    876\u001b[39m     array = numpy.array(array, order=order, dtype=dtype)\n\u001b[32m    877\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m     array = \u001b[43mnumpy\u001b[49m\u001b[43m.\u001b[49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[32m    881\u001b[39m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[32m    882\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m xp.asarray(array)\n",
      "\u001b[31mValueError\u001b[39m: could not convert string to float: 'Time'"
     ]
    }
   ],
   "source": [
    "# file names\n",
    "# ########################\n",
    "# file_name = \"wbc.mat\" \n",
    "# file_name = \"cardio.mat\"\n",
    "# file_name = \"thyroid.mat\"\n",
    "# file_name = \"musk.mat\"\n",
    "# file_name = \"shuttle.mat\"\n",
    "# file_name = \"satimage-2.mat\"\n",
    "# file_name = \"http.matv7\"\n",
    "# file_name = \"smtp.matv7\"\n",
    "# file_name = \"creditcard.csv\"\n",
    "\n",
    "classifiers = [\n",
    "    'HBOS',  # to be ignored, first run in loop slower\n",
    "    'HBOS',\n",
    "    'IForest',\n",
    "    'KNN',\n",
    "    'LOF',\n",
    "    'MCD',\n",
    "    'OCSVM',\n",
    "    'Perception',\n",
    "]\n",
    "\n",
    "metrics_df = None\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "\n",
    "    # loop over datasets in directory\n",
    "    for file_name in os.listdir(base_path):\n",
    "\n",
    "        dataset_name = file_name.split('.')[0]\n",
    "        file_path = base_path + file_name\n",
    "\n",
    "        if dataset_name == \"creditcard\":\n",
    "            df_temp = pd.read_csv(file_path, low_memory=False)\n",
    "            X_original = df_temp.iloc[:, :-1].values.astype(float)\n",
    "            y = df_temp.iloc[:, -1].values.astype(float)\n",
    "\n",
    "        else:\n",
    "            dataset_name, X_original, y = get_file_data(base_path, file_name)\n",
    "\n",
    "        if dataset_name is None:\n",
    "            continue\n",
    "\n",
    "        # scaling (very important to get right)\n",
    "        # scale to zero mean and unit standard deviation along each feature\n",
    "        sc = StandardScaler(with_mean=False)\n",
    "        sc.fit(X_original)\n",
    "        X = sc.transform(X_original)\n",
    "\n",
    "        # Apply each classifier to dataset\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore')\n",
    "\n",
    "            print('current file in progress ...: {}'.format(dataset_name))\n",
    "\n",
    "            metrics_temp = apply_classifiers(classifiers, dataset_name,\n",
    "                                             predict_data=X,\n",
    "                                             predict_labels=y,\n",
    "                                             train_data=X)\n",
    "\n",
    "        metrics_df = pd.concat([metrics_df, metrics_temp])\n",
    "\n",
    "    metrics_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T04:42:25.067383Z",
     "start_time": "2021-08-03T04:42:23.996883Z"
    }
   },
   "outputs": [],
   "source": [
    "# create dataframe for precision\n",
    "df_precision = metrics_df[['Dataset', 'Classifier', 'Precision']]\n",
    "df_precision = pd.pivot_table(df_precision, values = 'Precision', index=['Classifier'], columns='Dataset').reset_index()\n",
    "df_precision.columns.name = None\n",
    "\n",
    "cols = [col for col in df_precision.columns]\n",
    "formatdict = {}\n",
    "for col in cols: formatdict[col] = \"{:.3f}\"\n",
    "formatdict.pop('Classifier', None)\n",
    "\n",
    "sub = df_precision.columns.values.tolist()\n",
    "sub.remove('Classifier')\n",
    "sub\n",
    "\n",
    "df_precision = df_precision.style.hide().apply(highlight_max, subset=sub).format(formatdict)\n",
    "\n",
    "# img_title = \"Precision results\"\n",
    "# path_save = image_save_path + \"dataset_precision.png\"\n",
    "\n",
    "# dfi.export(df_precision,path_save)\n",
    "\n",
    "df_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T04:43:51.176250Z",
     "start_time": "2021-08-03T04:43:50.392906Z"
    }
   },
   "outputs": [],
   "source": [
    "# create dataframe for recall\n",
    "df_recall = metrics_df[['Dataset', 'Classifier', 'Recall']]\n",
    "df_recall = pd.pivot_table(df_recall, values = 'Recall', index=['Classifier'], columns='Dataset').reset_index()\n",
    "df_recall.columns.name = None\n",
    "\n",
    "cols = [col for col in df_recall.columns]\n",
    "formatdict = {}\n",
    "for col in cols: formatdict[col] = \"{:.2f}\"\n",
    "formatdict.pop('Classifier', None)\n",
    "\n",
    "sub = df_recall.columns.values.tolist()\n",
    "sub.remove('Classifier')\n",
    "sub\n",
    "\n",
    "df_recall = df_recall.style.hide().apply(highlight_max, subset=sub).format(formatdict)\n",
    "\n",
    "img_title = \"Recall results\"\n",
    "path_save = image_save_path + \"dataset_recall.png\"\n",
    "\n",
    "# dfi.export(df_recall,path_save)\n",
    "\n",
    "df_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T04:44:44.291587Z",
     "start_time": "2021-08-03T04:44:43.505516Z"
    }
   },
   "outputs": [],
   "source": [
    "# create dataframe for F1-score\n",
    "df_f1= metrics_df[['Dataset', 'Classifier', 'F1']]\n",
    "df_f1 = pd.pivot_table(df_f1, values = 'F1', index=['Classifier'], columns='Dataset').reset_index()\n",
    "df_f1.columns.name = None\n",
    "\n",
    "cols = [col for col in df_f1.columns]\n",
    "formatdict = {}\n",
    "for col in cols: formatdict[col] = \"{:.3f}\"\n",
    "formatdict.pop('Classifier', None)\n",
    "\n",
    "sub = df_f1.columns.values.tolist()\n",
    "sub.remove('Classifier')\n",
    "sub\n",
    "\n",
    "df_f1 = df_f1.style.hide().apply(highlight_max, subset=sub).format(formatdict)\n",
    "\n",
    "img_title = \"F1-score results\"\n",
    "path_save = image_save_path + \"dataset_f1-score.png\"\n",
    "\n",
    "# dfi.export(df_f1,path_save)\n",
    "\n",
    "df_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T04:44:15.845879Z",
     "start_time": "2021-08-03T04:44:15.061866Z"
    }
   },
   "outputs": [],
   "source": [
    "# create dataframe for Area under ROC curve\n",
    "df1= metrics_df[['Dataset', 'Classifier', 'AUC']]\n",
    "df1 = pd.pivot_table(df1, values = 'AUC', index=['Classifier'], columns='Dataset').reset_index()\n",
    "df1.columns.name = None\n",
    "\n",
    "cols = [col for col in df1.columns]\n",
    "formatdict = {}\n",
    "for col in cols: formatdict[col] = \"{:.2f}\"\n",
    "formatdict.pop('Classifier', None)\n",
    "\n",
    "sub = df1.columns.values.tolist()\n",
    "sub.remove('Classifier')\n",
    "sub\n",
    "\n",
    "df1 = df1.style.hide().apply(highlight_max, subset=sub).format(formatdict)\n",
    "\n",
    "img_title = \"F1-score results\"\n",
    "path_save = image_save_path + \"dataset_auc.png\"\n",
    "\n",
    "# dfi.export(df1,path_save)\n",
    "\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T04:45:06.233361Z",
     "start_time": "2021-08-03T04:45:05.431576Z"
    }
   },
   "outputs": [],
   "source": [
    "# create dataframe for total training and prediction time\n",
    "df1= metrics_df[['Dataset', 'Classifier', 'Runtime']]\n",
    "df1 = pd.pivot_table(df1, values = 'Runtime', index=['Classifier'], columns='Dataset').reset_index()\n",
    "df1.columns.name = None\n",
    "\n",
    "cols = [col for col in df1.columns]\n",
    "formatdict = {}\n",
    "for col in cols: formatdict[col] = \"{:.4f}\"\n",
    "formatdict.pop('Classifier', None)\n",
    "\n",
    "sub = df1.columns.values.tolist()\n",
    "sub.remove('Classifier')\n",
    "sub\n",
    "\n",
    "df1 = df1.style.hide().apply(highlight_min, subset=sub).format(formatdict)\n",
    "\n",
    "path_save = image_save_path + \"dataset_total_time.png\"\n",
    "\n",
    "# dfi.export(df1,path_save)\n",
    "\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "perceptionenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
