{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@author Nassir Mohammad\n",
    "\n",
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T19:22:16.085670Z",
     "start_time": "2021-08-02T19:22:16.078580Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "sys.path.append('../')\n",
    "sys.path.append('../scripts')\n",
    "\n",
    "import warnings\n",
    "from perception_nassir import Perception\n",
    "\n",
    "import dataframe_image as dfi\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scripts.utilities import apply_classifiers\n",
    "from scripts.utilities import get_file_data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scripts.rendering_functions import highlight_max, highlight_min\n",
    "\n",
    "image_save_path = ''\n",
    "image_save_switch = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper 1 datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note on creditcard.csv**  \n",
    "> The file `creditcard.csv` is not included in this repository under `data/ODDS_multivariate` due to its large size.  \n",
    "> Download it from: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud and place it inside `data/ODDS_multivariate/`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T19:24:54.498344Z",
     "start_time": "2021-08-02T19:24:52.418531Z"
    }
   },
   "outputs": [],
   "source": [
    "# make table for dataset, # dimensions, # samples, # percentage of anomalies\n",
    "\n",
    "base_path = \"../data/ODDS_multivariate/\"\n",
    "data_properties_df = None\n",
    "\n",
    "# loop over datasets in directory\n",
    "for file_name in os.listdir(base_path):\n",
    " \n",
    "    dataset_name = file_name.split('.')[0]\n",
    "    file_path = base_path + file_name\n",
    "\n",
    "    if dataset_name == \"creditcard\":\n",
    "        df_temp = pd.read_csv(file_path, low_memory=False)\n",
    "        X_original = df_temp.iloc[:, :-1].values.astype(float)\n",
    "        y = df_temp.iloc[:, -1].values.astype(float)\n",
    "    else:\n",
    "        dataset_name, X_original, y = get_file_data(base_path, file_name)\n",
    "\n",
    "    if dataset_name is None:\n",
    "        continue\n",
    "\n",
    "    # write dataset summary to dataframe\n",
    "    data_properties_temp = pd.DataFrame({\n",
    "        'Name': [dataset_name],\n",
    "        '# examples': [X_original.shape[0]],\n",
    "        '# features': [X_original.shape[1]],\n",
    "        # '# anomalies': [y.sum()],\n",
    "        '% anomalies': [round(y.sum() / X_original.shape[0] * 100, 2)],\n",
    "    })\n",
    "\n",
    "    data_properties_df = pd.concat(\n",
    "        [data_properties_df, data_properties_temp]).reset_index(drop=True)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T19:25:36.071578Z",
     "start_time": "2021-08-02T19:25:34.988414Z"
    }
   },
   "outputs": [],
   "source": [
    "img_title = \"Dataset properties\"\n",
    "path_save = image_save_path + \"dataset_properties.png\"\n",
    "\n",
    "# order the dataset rows by name\n",
    "data_properties_df = data_properties_df.sort_values(by=['Name']).reset_index(drop=True)\n",
    "\n",
    "data_properties_df_styled = data_properties_df.style.format({'% anomalies': \"{:.2f}\"}).hide()\n",
    "\n",
    "#data_properties_df_styled = data_properties_df.style.hide_index()\n",
    "\n",
    "#dfi.export(data_properties_df,path_save)\n",
    "\n",
    "data_properties_df_styled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T04:32:36.346095Z",
     "start_time": "2021-08-02T19:31:20.182967Z"
    },
    "code_folding": [
     73,
     83,
     96,
     106
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# file names\n",
    "# ########################\n",
    "# file_name = \"wbc.mat\" \n",
    "# file_name = \"cardio.mat\"\n",
    "# file_name = \"thyroid.mat\"\n",
    "# file_name = \"musk.mat\"\n",
    "# file_name = \"shuttle.mat\"\n",
    "# file_name = \"satimage-2.mat\"\n",
    "# file_name = \"http.matv7\"\n",
    "# file_name = \"smtp.matv7\"\n",
    "# file_name = \"creditcard.csv\"\n",
    "\n",
    "classifiers = [\n",
    "    'HBOS',  # to be ignored, first run in loop slower\n",
    "    'HBOS',\n",
    "    'IForest',\n",
    "    'KNN',\n",
    "    'LOF',\n",
    "    'MCD',\n",
    "    'OCSVM',\n",
    "    'Perception',\n",
    "]\n",
    "\n",
    "metrics_df = None\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "\n",
    "    # loop over datasets in directory\n",
    "    for file_name in os.listdir(base_path):\n",
    "\n",
    "        dataset_name = file_name.split('.')[0]\n",
    "        file_path = base_path + file_name\n",
    "\n",
    "        if dataset_name == \"creditcard\":\n",
    "            df_temp = pd.read_csv(file_path, low_memory=False)\n",
    "            X_original = df_temp.iloc[:, :-1].values.astype(float)\n",
    "            y = df_temp.iloc[:, -1].values.astype(float)\n",
    "\n",
    "        else:\n",
    "            dataset_name, X_original, y = get_file_data(base_path, file_name)\n",
    "\n",
    "        if dataset_name is None:\n",
    "            continue\n",
    "\n",
    "        # scaling (very important to get right)\n",
    "        # scale to zero mean and unit standard deviation along each feature\n",
    "        sc = StandardScaler(with_mean=False)\n",
    "        sc.fit(X_original)\n",
    "        X = sc.transform(X_original)\n",
    "\n",
    "        # Apply each classifier to dataset\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore')\n",
    "\n",
    "            print('current file in progress ...: {}'.format(dataset_name))\n",
    "\n",
    "            metrics_temp = apply_classifiers(classifiers, dataset_name,\n",
    "                                             predict_data=X,\n",
    "                                             predict_labels=y,\n",
    "                                             train_data=X)\n",
    "\n",
    "        metrics_df = pd.concat([metrics_df, metrics_temp])\n",
    "\n",
    "    metrics_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T04:42:25.067383Z",
     "start_time": "2021-08-03T04:42:23.996883Z"
    }
   },
   "outputs": [],
   "source": [
    "# create dataframe for precision\n",
    "df_precision = metrics_df[['Dataset', 'Classifier', 'Precision']]\n",
    "df_precision = pd.pivot_table(df_precision, values = 'Precision', index=['Classifier'], columns='Dataset').reset_index()\n",
    "df_precision.columns.name = None\n",
    "\n",
    "cols = [col for col in df_precision.columns]\n",
    "formatdict = {}\n",
    "for col in cols: formatdict[col] = \"{:.3f}\"\n",
    "formatdict.pop('Classifier', None)\n",
    "\n",
    "sub = df_precision.columns.values.tolist()\n",
    "sub.remove('Classifier')\n",
    "sub\n",
    "\n",
    "df_precision = df_precision.style.hide().apply(highlight_max, subset=sub).format(formatdict)\n",
    "\n",
    "# img_title = \"Precision results\"\n",
    "# path_save = image_save_path + \"dataset_precision.png\"\n",
    "\n",
    "# dfi.export(df_precision,path_save)\n",
    "\n",
    "df_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T04:43:51.176250Z",
     "start_time": "2021-08-03T04:43:50.392906Z"
    }
   },
   "outputs": [],
   "source": [
    "# create dataframe for recall\n",
    "df_recall = metrics_df[['Dataset', 'Classifier', 'Recall']]\n",
    "df_recall = pd.pivot_table(df_recall, values = 'Recall', index=['Classifier'], columns='Dataset').reset_index()\n",
    "df_recall.columns.name = None\n",
    "\n",
    "cols = [col for col in df_recall.columns]\n",
    "formatdict = {}\n",
    "for col in cols: formatdict[col] = \"{:.2f}\"\n",
    "formatdict.pop('Classifier', None)\n",
    "\n",
    "sub = df_recall.columns.values.tolist()\n",
    "sub.remove('Classifier')\n",
    "sub\n",
    "\n",
    "df_recall = df_recall.style.hide().apply(highlight_max, subset=sub).format(formatdict)\n",
    "\n",
    "img_title = \"Recall results\"\n",
    "path_save = image_save_path + \"dataset_recall.png\"\n",
    "\n",
    "# dfi.export(df_recall,path_save)\n",
    "\n",
    "df_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T04:44:44.291587Z",
     "start_time": "2021-08-03T04:44:43.505516Z"
    }
   },
   "outputs": [],
   "source": [
    "# create dataframe for F1-score\n",
    "df_f1= metrics_df[['Dataset', 'Classifier', 'F1']]\n",
    "df_f1 = pd.pivot_table(df_f1, values = 'F1', index=['Classifier'], columns='Dataset').reset_index()\n",
    "df_f1.columns.name = None\n",
    "\n",
    "cols = [col for col in df_f1.columns]\n",
    "formatdict = {}\n",
    "for col in cols: formatdict[col] = \"{:.3f}\"\n",
    "formatdict.pop('Classifier', None)\n",
    "\n",
    "sub = df_f1.columns.values.tolist()\n",
    "sub.remove('Classifier')\n",
    "sub\n",
    "\n",
    "df_f1 = df_f1.style.hide().apply(highlight_max, subset=sub).format(formatdict)\n",
    "\n",
    "img_title = \"F1-score results\"\n",
    "path_save = image_save_path + \"dataset_f1-score.png\"\n",
    "\n",
    "# dfi.export(df_f1,path_save)\n",
    "\n",
    "df_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T04:44:15.845879Z",
     "start_time": "2021-08-03T04:44:15.061866Z"
    }
   },
   "outputs": [],
   "source": [
    "# create dataframe for Area under ROC curve\n",
    "df1= metrics_df[['Dataset', 'Classifier', 'AUC']]\n",
    "df1 = pd.pivot_table(df1, values = 'AUC', index=['Classifier'], columns='Dataset').reset_index()\n",
    "df1.columns.name = None\n",
    "\n",
    "cols = [col for col in df1.columns]\n",
    "formatdict = {}\n",
    "for col in cols: formatdict[col] = \"{:.2f}\"\n",
    "formatdict.pop('Classifier', None)\n",
    "\n",
    "sub = df1.columns.values.tolist()\n",
    "sub.remove('Classifier')\n",
    "sub\n",
    "\n",
    "df1 = df1.style.hide().apply(highlight_max, subset=sub).format(formatdict)\n",
    "\n",
    "img_title = \"F1-score results\"\n",
    "path_save = image_save_path + \"dataset_auc.png\"\n",
    "\n",
    "# dfi.export(df1,path_save)\n",
    "\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T04:45:06.233361Z",
     "start_time": "2021-08-03T04:45:05.431576Z"
    }
   },
   "outputs": [],
   "source": [
    "# create dataframe for total training and prediction time\n",
    "df1= metrics_df[['Dataset', 'Classifier', 'Runtime']]\n",
    "df1 = pd.pivot_table(df1, values = 'Runtime', index=['Classifier'], columns='Dataset').reset_index()\n",
    "df1.columns.name = None\n",
    "\n",
    "cols = [col for col in df1.columns]\n",
    "formatdict = {}\n",
    "for col in cols: formatdict[col] = \"{:.4f}\"\n",
    "formatdict.pop('Classifier', None)\n",
    "\n",
    "sub = df1.columns.values.tolist()\n",
    "sub.remove('Classifier')\n",
    "sub\n",
    "\n",
    "df1 = df1.style.hide().apply(highlight_min, subset=sub).format(formatdict)\n",
    "\n",
    "path_save = image_save_path + \"dataset_total_time.png\"\n",
    "\n",
    "# dfi.export(df1,path_save)\n",
    "\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "perceptionenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
